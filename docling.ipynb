{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:02:21.987428Z",
     "start_time": "2025-09-03T04:02:21.985330Z"
    }
   },
   "cell_type": "code",
   "source": "from pathlib import Path",
   "id": "a2166fe78e336556",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:02:22.051556Z",
     "start_time": "2025-09-03T04:02:22.043175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core import documents\n",
    "BASE = Path(\"/home/pibezx/Documents/Proyectos\")\n",
    "PDF_DIR = BASE\n",
    "PROCESSED_DIR = BASE / \"data\" / \"processed\"\n",
    "INDEX_DIR = BASE / \"index\" / \"chroma_autos\"\n",
    "\n",
    "for p in [PROCESSED_DIR, INDEX_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SKIP_DIR_NAMES = {\"data\", \"index\", \".git\", \".ipynb_checkpoints\", \".idea\", \".venv\", \"env\", \"venv\"}\n",
    "\n",
    "def should_skip(path: Path) -> bool:\n",
    "    return any(part.startswith(\".\") or part in SKIP_DIR_NAMES for part in path.parts)\n",
    "\n",
    "# Descubrir PDFs\n",
    "candidates = []\n",
    "for p in PDF_DIR.rglob(\"*.pdf\"):\n",
    "    if should_skip(p):\n",
    "        continue\n",
    "    if PROCESSED_DIR in p.parents:\n",
    "        continue\n",
    "    candidates.append(p.resolve())\n",
    "\n",
    "print(\"PDFs detectados:\", len(candidates))\n",
    "for i, p in enumerate(candidates, 1):\n",
    "    print(f\"{i:>2}. {p} | exists={p.exists()}\")\n"
   ],
   "id": "b655af260dab5430",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs detectados: 2\n",
      " 1. /home/pibezx/Documents/Proyectos/Toyota/CATALOGO_COROLLA_PERU.pdf | exists=True\n",
      " 2. /home/pibezx/Documents/Proyectos/Volkswagen/Ficha-Tecnica-Amarok-2025.pdf | exists=True\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:02.011535Z",
     "start_time": "2025-09-03T04:02:22.098744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json,os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "SKIP_DIR_NAMES = {\"data\", \"index\", \".git\", \".ipynb_checkpoints\", \".idea\", \".venv\", \"env\", \"venv\"}\n",
    "\n",
    "class ParseResult(BaseModel):\n",
    "    pdf_path: str\n",
    "    md_path: str\n",
    "    json_meta_path: str\n",
    "    chars : int\n",
    "    used : str   #docling\n",
    "\n",
    "def rel_to_base(path: Path, base: Path) -> Path:\n",
    "    # Devuelve path relativo a base sin reventar si no es subpath directo\n",
    "    try:\n",
    "        return path.relative_to(base)\n",
    "    except Exception:\n",
    "        return Path(os.path.relpath(path, base))\n",
    "\n",
    "def convert_pdf_docling(pdf_path : Path):\n",
    "    conv = DocumentConverter()\n",
    "    res = conv.convert(str(pdf_path))\n",
    "    md_text = res.document.export_to_markdown()\n",
    "    meta = res.document.as_dict() if hasattr(res.document, \"as_dict\") else {\"note\":\"no-as_dict\"}\n",
    "    return md_text, meta\n",
    "\n",
    "def convert_pdf(pdf_path : Path,out_md: Path, out_json: Path):\n",
    "    md_text, meta = convert_pdf_docling(pdf_path)\n",
    "    out_md.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_md.write_text(md_text, encoding=\"utf-8\")\n",
    "    out_json.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return ParseResult(pdf_path=str(pdf_path), md_path=str(out_md), json_meta_path=str(out_json),\n",
    "                       chars=len(md_text), used=\"docling\")\n",
    "\n",
    "def ingest_all(pdf_list: List[Path], processed_root: Path) -> List[ParseResult]:\n",
    "    results = []\n",
    "    for pdf in sorted(pdf_list):          # <<< iteramos por CADA Path\n",
    "        if not pdf.exists():\n",
    "            print(f\"[WARN] No existe: {pdf}\")\n",
    "            continue\n",
    "        rel = rel_to_base(pdf, PDF_DIR)   # relativo a la raíz del proyecto\n",
    "        out_md = processed_root / rel.with_suffix(\".md\")\n",
    "        out_json = processed_root / rel.with_suffix(\".json\")\n",
    "        r = convert_pdf(pdf, out_md, out_json)\n",
    "        print(f\"✓ docling  {rel} -> data/processed/{rel.with_suffix('.md')} ({r.chars} chars)\")\n",
    "        results.append(r)\n",
    "    return results\n",
    "\n",
    "\n",
    "ingest_summary = ingest_all(candidates, PROCESSED_DIR)\n",
    "print(f\"\\nListo {len(ingest_summary)} PDFs procesados.\")"
   ],
   "id": "7b63ea604fab784a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ docling  Toyota/CATALOGO_COROLLA_PERU.pdf -> data/processed/Toyota/CATALOGO_COROLLA_PERU.md (39324 chars)\n",
      "✓ docling  Volkswagen/Ficha-Tecnica-Amarok-2025.pdf -> data/processed/Volkswagen/Ficha-Tecnica-Amarok-2025.md (39315 chars)\n",
      "\n",
      "Listo 2 PDFs procesados.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:02.158863Z",
     "start_time": "2025-09-03T04:03:02.104713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import torch\n",
    "\n",
    "# Para aprovechar la estructura Markdown\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=400,chunk_overlap=100\n",
    ")\n",
    "\n",
    "#ahora viene lo chido\n",
    "def load_md_documents(processed_root: Path):\n",
    "    docs = []\n",
    "    for md in processed_root.rglob(\"*.md\"):\n",
    "        docs.extend(TextLoader(str(md), encoding=\"utf-8\").load())\n",
    "    return docs\n",
    "\n",
    "docs = load_md_documents(PROCESSED_DIR)\n",
    "chunks = markdown_splitter.split_documents(docs)\n",
    "\n"
   ],
   "id": "555ae1e85ecb6791",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:02.167042Z",
     "start_time": "2025-09-03T04:03:02.164084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create a context manager to help with GPU memory management\n",
    "class GPUMemoryManager:\n",
    "    def __enter__(self):\n",
    "        # Nothing to do on enter\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        # Clean up CUDA cache on exit if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def clear(self):\n",
    "        # Manually clear GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Create a global instance for use throughout the notebook\n",
    "gpu_memory_mgr = GPUMemoryManager()"
   ],
   "id": "618e4aaf9a0659e1",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:05.332872Z",
     "start_time": "2025-09-03T04:03:02.211228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure tokenizer to avoid warnings\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()  # Reduce warning messages\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"intfloat/multilingual-e5-base\",\n",
    "    model_kwargs = {\"device\": device}\n",
    ")\n"
   ],
   "id": "d9358568cdc2562a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:06.176148Z",
     "start_time": "2025-09-03T04:03:05.387050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents= chunks,\n",
    "    embedding = embeddings,\n",
    "    persist_directory=str(INDEX_DIR)\n",
    ")\n",
    "#vector_db.persist()\n",
    "len(chunks)"
   ],
   "id": "c9a77457960024",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:08.759354Z",
     "start_time": "2025-09-03T04:03:06.229177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "#vector_db = Chroma(persist_directory=str(INDEX_DIR), embedding_function=embeddings) #error de versiones deprecated\n",
    "\n",
    "base_docs = load_md_documents(PROCESSED_DIR)\n",
    "base_chunks = markdown_splitter.split_documents(base_docs)\n",
    "bm25 = BM25Retriever.from_documents(base_chunks)\n",
    "bm25.k = 12\n",
    "\n",
    "vec_retriever = vector_db.as_retriever(search_kwargs={\"k\":12})\n",
    "\n",
    "reranker = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=(device == \"cuda\"))\n",
    "\n",
    "def retrieve(query: str, topk:int=2):\n",
    "    vec_docs = vec_retriever.invoke(query)\n",
    "    bm_docs = bm25.invoke(query)\n",
    "    # merge + dedupe por (contenido corto, fuente)\n",
    "    pool, seen = [], set()\n",
    "    for d in vec_docs + bm_docs:\n",
    "        key = (d.page_content[:200], d.metadata.get(\"source\"))\n",
    "        if key not in seen:\n",
    "            pool.append(d); seen.add(key)\n",
    "    # rerank\n",
    "    pairs = [[query, d.page_content] for d in pool]\n",
    "    scores = reranker.compute_score(pairs, normalize=True)\n",
    "    order = sorted(range(len(pool)), key=lambda i: -scores[i])\n",
    "    return [pool[i] for i in order[:topk]]"
   ],
   "id": "4028e2f9a2a761e7",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:08.834103Z",
     "start_time": "2025-09-03T04:03:08.816872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model = \"llama3.2:3b\")"
   ],
   "id": "f7187058afd08e27",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:03:09.045869Z",
     "start_time": "2025-09-03T04:03:08.862934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "cliente = OpenAI(api_key=\"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\", base_url=\"https://openrouter.ai/api/v1\")"
   ],
   "id": "8dd05473e5280379",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:39:52.981181Z",
     "start_time": "2025-09-03T04:39:52.976720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def format_sources(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\",\"\")\n",
    "        if src:\n",
    "            try:\n",
    "                src_rel = str(Path(src)).replace(str(PROCESSED_DIR)+\"/\", \"\")\n",
    "            except Exception:\n",
    "                src_rel = src\n",
    "            out.append(src_rel)\n",
    "    return list(dict.fromkeys(out))  # únicos\n",
    "\n",
    "def ask(question: str):\n",
    "    docs = retrieve(question, topk=1)\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content[:3000] for d in docs)\n",
    "    \n",
    "    # Usar la API externa en lugar de Ollama\n",
    "    try:\n",
    "        response = cliente.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-r1:free\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Eres un asesor experto en autos para Perú y responde en Español siempre. \"\n",
    "                                            \"Responde SOLO con el contexto proporcionado. \"\n",
    "                                            \"Especifica versión/año si aplica. \"\n",
    "                                            \"Si no está en el contexto, di claramente que no tienes ese dato.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Contexto:\\n{context}\\n\\nPregunta: {question}\"}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        answer = f\"Error al procesar la consulta: {str(e)}\"\n",
    "    \n",
    "    cites = format_sources(docs)\n",
    "    return answer, cites\n"
   ],
   "id": "a3db2abfc3b434d7",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:40:08.891875Z",
     "start_time": "2025-09-03T04:39:54.409348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q = \"Cuales son las medidas o dimensiones que tiene la Volkswagen Amarok? \"\n",
    "ans, cites = ask(q)\n",
    "print(ans, \"\\n\\nFuentes:\")\n",
    "for c in cites:\n",
    "    print(\" -\", c)\n"
   ],
   "id": "e768253927cad086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según el contexto proporcionado, las dimensiones de la Volkswagen Amarok mencionadas son:\n",
      "\n",
      "- **Altura**: 1,455 mm  \n",
      "- **Ancho**: 1,780 mm  \n",
      "- **Longitud**: 4,630 mm  \n",
      "- **Distancia al suelo**: 148 mm  \n",
      "\n",
      "*Nota*: Los asteriscos (*) en \"Altura\" y \"Distancia al suelo\" están presentes en el contexto original, pero no se incluye información adicional sobre su significado. No se especifica la versión o año del modelo al que corresponden estas medidas. Si necesitas detalles técnicos adicionales (como peso, batalla, capacidad de carga, etc.), no están disponibles en el contexto compartido. \n",
      "\n",
      "Fuentes:\n",
      " - Toyota/CATALOGO_COROLLA_PERU.md\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
