{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T21:07:18.254764Z",
     "start_time": "2025-08-10T21:07:18.247059Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain_core import documents\n",
    "\n",
    "BASE = Path(\"/home/pibezx/Documents/Proyectos\")\n",
    "PDF_DIR = BASE\n",
    "PROCESSED_DIR = BASE / \"data\" / \"processed\"\n",
    "INDEX_DIR = BASE / \"index\" / \"chroma_autos\"\n",
    "\n",
    "for p in [PROCESSED_DIR, INDEX_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SKIP_DIR_NAMES = {\"data\", \"index\", \".git\", \".ipynb_checkpoints\", \".idea\", \".venv\", \"env\", \"venv\"}\n",
    "\n",
    "def should_skip(path: Path) -> bool:\n",
    "    return any(part.startswith(\".\") or part in SKIP_DIR_NAMES for part in path.parts)\n",
    "\n",
    "# Descubrir PDFs\n",
    "candidates = []\n",
    "for p in PDF_DIR.rglob(\"*.pdf\"):\n",
    "    if should_skip(p):\n",
    "        continue\n",
    "    if PROCESSED_DIR in p.parents:\n",
    "        continue\n",
    "    candidates.append(p.resolve())\n",
    "\n",
    "print(\"PDFs detectados:\", len(candidates))\n",
    "for i, p in enumerate(candidates, 1):\n",
    "    print(f\"{i:>2}. {p} | exists={p.exists()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs detectados: 2\n",
      " 1. /home/pibezx/Documents/Proyectos/Toyota/CATALOGO_COROLLA_PERU.pdf | exists=True\n",
      " 2. /home/pibezx/Documents/Proyectos/Volkswagen/Ficha-Tecnica-Amarok-2025.pdf | exists=True\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:08:36.369275Z",
     "start_time": "2025-08-10T21:07:29.542808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json,os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "SKIP_DIR_NAMES = {\"data\", \"index\", \".git\", \".ipynb_checkpoints\", \".idea\", \".venv\", \"env\", \"venv\"}\n",
    "\n",
    "class ParseResult(BaseModel):\n",
    "    pdf_path: str\n",
    "    md_path: str\n",
    "    json_meta_path: str\n",
    "    chars : int\n",
    "    used : str   #docling\n",
    "\n",
    "def rel_to_base(path: Path, base: Path) -> Path:\n",
    "    # Devuelve path relativo a base sin reventar si no es subpath directo\n",
    "    try:\n",
    "        return path.relative_to(base)\n",
    "    except Exception:\n",
    "        return Path(os.path.relpath(path, base))\n",
    "\n",
    "def convert_pdf_docling(pdf_path : Path):\n",
    "    conv = DocumentConverter()\n",
    "    res = conv.convert(str(pdf_path))\n",
    "    md_text = res.document.export_to_markdown()\n",
    "    meta = res.document.as_dict() if hasattr(res.document, \"as_dict\") else {\"note\":\"no-as_dict\"}\n",
    "    return md_text, meta\n",
    "\n",
    "def convert_pdf(pdf_path : Path,out_md: Path, out_json: Path):\n",
    "    md_text, meta = convert_pdf_docling(pdf_path)\n",
    "    out_md.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_md.write_text(md_text, encoding=\"utf-8\")\n",
    "    out_json.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return ParseResult(pdf_path=str(pdf_path), md_path=str(out_md), json_meta_path=str(out_json),\n",
    "                       chars=len(md_text), used=\"docling\")\n",
    "\n",
    "def ingest_all(pdf_list: List[Path], processed_root: Path) -> List[ParseResult]:\n",
    "    results = []\n",
    "    for pdf in sorted(pdf_list):          # <<< iteramos por CADA Path\n",
    "        if not pdf.exists():\n",
    "            print(f\"[WARN] No existe: {pdf}\")\n",
    "            continue\n",
    "        rel = rel_to_base(pdf, PDF_DIR)   # relativo a la raíz del proyecto\n",
    "        out_md = processed_root / rel.with_suffix(\".md\")\n",
    "        out_json = processed_root / rel.with_suffix(\".json\")\n",
    "        r = convert_pdf(pdf, out_md, out_json)\n",
    "        print(f\"✓ docling  {rel} -> data/processed/{rel.with_suffix('.md')} ({r.chars} chars)\")\n",
    "        results.append(r)\n",
    "    return results\n",
    "\n",
    "ingest_summary = ingest_all(candidates, PROCESSED_DIR)\n",
    "print(f\"\\nListo. {len(ingest_summary)} PDFs procesados.\")\n"
   ],
   "id": "607970b413e548db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ docling  Toyota/CATALOGO_COROLLA_PERU.pdf -> data/processed/Toyota/CATALOGO_COROLLA_PERU.md (39324 chars)\n",
      "✓ docling  Volkswagen/Ficha-Tecnica-Amarok-2025.pdf -> data/processed/Volkswagen/Ficha-Tecnica-Amarok-2025.md (39315 chars)\n",
      "\n",
      "Listo. 2 PDFs procesados.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:15:03.369758Z",
     "start_time": "2025-08-10T21:15:03.039591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import torch\n",
    "\n",
    "#ahora viene lo chido\n",
    "def load_md_documents(processed_root: Path):\n",
    "    docs = []\n",
    "    for md in processed_root.rglob(\"*.md\"):\n",
    "        docs.extend(TextLoader(str(md), encoding=\"utf-8\").load())\n",
    "    return docs\n",
    "\n",
    "docs = load_md_documents(PROCESSED_DIR)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n"
   ],
   "id": "7c46e4826e8f15c2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:15:05.389485Z",
     "start_time": "2025-08-10T21:15:05.387012Z"
    }
   },
   "cell_type": "code",
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "id": "463bc906ac652142",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:16:33.342Z",
     "start_time": "2025-08-10T21:15:59.183073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"intfloat/multilingual-e5-base\",\n",
    "    model_kwargs = {\"device\": device}\n",
    ")\n"
   ],
   "id": "c7debf797841c4f9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:17:59.440678Z",
     "start_time": "2025-08-10T21:17:58.730132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents= chunks,\n",
    "    embedding = embeddings,\n",
    "    persist_directory=str(INDEX_DIR)\n",
    ")\n",
    "#vector_db.persist()\n",
    "len(chunks)"
   ],
   "id": "f84eedbfdfcddb50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T21:23:26.827370Z",
     "start_time": "2025-08-10T21:22:26.471326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "#vector_db = Chroma(persist_directory=str(INDEX_DIR), embedding_function=embeddings) #error de versiones deprecated\n",
    "\n",
    "base_docs = load_md_documents(PROCESSED_DIR)\n",
    "base_chunks = splitter.split_documents(base_docs)\n",
    "bm25 = BM25Retriever.from_documents(base_chunks)\n",
    "bm25.k = 12\n",
    "\n",
    "\n",
    "vec_retriever = vector_db.as_retriever(search_kwargs={\"k\":12})\n",
    "\n",
    "reranker = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=(device == \"cuda\"))\n"
   ],
   "id": "f033047d2892a2aa",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f7187058afd08e27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
